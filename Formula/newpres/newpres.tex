\documentclass{beamer}

\usetheme{CambridgeUS}
\usecolortheme{orchid}

\usepackage{wrapfig}
\usepackage{siunitx}
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{adjustbox}
\usepackage{nicefrac}

% Tikz
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows,calc,intersections}
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\pgfplotsset{compat=1.8}

\input{custom}

\begin{document}

\title[Reduced Basis Methods]{
  Spline-based Compatible Reduced Basis Methods for Flow Problems
}
\author[A. Rasheed]{
  A.~Rasheed\inst{1,2} \and
  E.~Fonn\inst{2} \and
  E.~H.~van Brummelen\inst{3} \and
  T.~Kvamsdal\inst{4}
}
\institute[SINTEF]{
  \inst{1}%
  \url{adil.rasheed@sintef.no}
  \and \inst{2}%
  Mathematics and Cybernetics, SINTEF Digital
  \and \inst{3}%
  Department of Mechanical Engineering, TU/e
  \and \inst{4}%
  Department of Mathematical Sciences, NTNU
}
\date[Hybrid Analytics]{}

\definecolor{darkblue}{HTML}{00688B}
\definecolor{darkgreen}{HTML}{6E8B3D}
\definecolor{cadet}{HTML}{DAE1FF}
\definecolor{salmon}{HTML}{FFB08A}

\titlegraphic{\includegraphics[width=0.3\textwidth]{figs/sintef}}

 \begin{frame}
   \titlepage
 \end{frame}

\begin{frame}
\frametitle{Logisitc Regression}
$J(\theta)=\frac{1}{m}\sum\limits_{i=1}^{m}Cost(h_{\theta}(x^{(i)})-y^{(i)})$\\

$Cost(h_{\theta}(x)-y)=-log(h_{\theta})$ if $y=1$\\

$Cost(h_{\theta}(x)-y)=-log(1-h_{\theta})$ \\

$J(\theta)=-\frac{1}{m}\sum\limits_{i=1}^{m}[y^{(i)}log(h_{\theta}(x^{(i)}))  (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]$
\end{frame}

\begin{frame}
	\frametitle{Decision Tree}
	$Entropy=\sum\limits_{i}-p_{i}log_{2}p_{i}$ \\
	$p_{i}$ is the probability of class $i$ \\ \vspace{0.5cm} 
	Information Gain = $Entropy_{parent}$-Average $Entropy_{children}$
\end{frame}



  \begin{frame}
  	\frametitle{Batch Gradient Descent vs Stochastic Gradient Descent}
  	\begin{columns}
  		\begin{column}{0.5\textwidth}
  			$J_{train}(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}$\\ \vspace{1cm}
  			Repeat \{ \vspace{0.5cm}
  			$\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}$\\
  			for every $j=0,...,n$ \vspace{0.5cm} \\
  			\}
  		\end{column}
  		\begin{column}{0.5\textwidth}  %%<--- here
			${cost(\theta,(x^{(i)},y^{(i)})=\frac{1}{2}(h_{\theta}(x^{(i)})-y^{(i)})^{2}}$\\
			$J_{train}(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}cost(\theta,(x^{(i)},y^{(i)})$\\ \vspace{0.5cm}		
			Repeat \{\\ \vspace{0.5cm}
			for i =1,....m \{
			$\theta_{j}:=\theta_{j}-\alpha(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}$\\
			  \vspace{0.5cm} \}\\
			\}			
  		\end{column}
  	\end{columns}
  \end{frame}
 
 \begin{frame}
 	\frametitle{Deep Neural Network}
 	$\textbf{h}^{(1)}=\sigma(\mathbf{\Theta^{1}} \mathbf{X}+\textbf{b}^{1})$ \\
 	$\textbf{h}^{(2)}=\sigma(\mathbf{\Theta^{2}} \textbf{h}^{(1)}+\textbf{b}^{2})$ \\
 	$\vdots$ \\
 	$\textbf{h}^{(n-1)}=\sigma(\mathbf{\Theta^{n-1}} \textbf{h}^{(n-2)}+\textbf{b}^{n-1})$ \\
 	$\textbf{H}_{\Theta}(\textbf{X})=\sigma(\mathbf{\Theta^{n}} \textbf{h}^{(n-1)}+\textbf{b}^{n})$ \\
 \end{frame} 
 
%  \begin{frame}
%  	\frametitle{Deep Neural Network}
%  	$\textbf{X}(m,n), \mathbf{\Theta^{1}}(n,p^{2}), b_{1}(m,1) \\
%  	$\textbf{h}^{(1)}(m,p^{2}), \mathbf{\Theta^{2}}(p^{2},p^{3}), b_{2}(m,1)$\\
%  	$\textbf{h}^{(2)}(m,p^{3}), \mathbf{\Theta^{3}}(p^{3},p^{4}), b_{3}(m,1)$\\ 
%  	$\vdots$\\
%  	$\textbf{h}^{(n-1)}(m,p^{n}), \mathbf{\Theta^{n-1}}(p^{n-1},p^{n}), b_{n-1}(m,1)$\\ \vspace{0.5cm}	
%  	$\textbf{h}^{(1)}=\sigma(\mathbf{X}\mathbf{\Theta^{1}} +b^{1})$ \\
%  	$\textbf{h}^{(2)}=\sigma(\textbf{h}^{(1)}\mathbf{\Theta^{2}} +b^{2})$ \\
%    $\textbf{h}^{(3)}=\sigma(\textbf{h}^{(2)}\mathbf{\Theta^{3}} +b^{3})$ \\
%    $\vdots$\\
%    $\textbf{h}^{(n-2)}=\sigma(\textbf{h}^{(n-3)}\mathbf{\Theta^{n-2}} +b^{n-2})$ \\
%    $\textbf{H}_{\mathbf{\Theta}}=\sigma(\textbf{h}^{(n-2)}\mathbf{\Theta^{n-1}} +b^{n-1})$ \\
%  $p^{i}$ is the number of nodes in the $i^{th}$ layer. Thus $p^{n}$ is the number of output nodes,
%  	$n$ is the number of layers in the network (input+hidden+output)
%  \end{frame}  
  
  
    \begin{frame}
    \frametitle{Deep Neural Network}
    $\textbf{X}(m,q), \mathbf{\Theta^{1}}(q+1,p^{(2)})$ \\
    $\textbf{h}^{(1)}(m,p^{(2)}), \mathbf{\Theta^{2}}(p^{(2)}+1,p^{(3)})$\\
    $\textbf{h}^{(2)}(m,p^{(3)}), \mathbf{\Theta^{3}}(p^{(3)}+1,p^{(4)})$\\ 
    $\vdots$\\
    $\textbf{h}^{(n-1)}(m,p^{n}), \mathbf{\Theta^{n-1}}(p^{n-1}+1,p^{n}), b_{n-1}(m,1)$\\ \vspace{0.5cm}	
\end{frame} 

    \begin{frame}
    	\frametitle{Deep Neural Network}
    	$\textbf{X}=\textbf{[1 X]}$\\
    	$\textbf{h}^{(1)}=\sigma(\mathbf{X}\mathbf{\Theta^{1}})$ \\
    	$\textbf{h}^{(1)}=\mathbf{[1 \hspace{0.2cm}h^{(1)}]}$\\
    	$\textbf{h}^{(2)}=\sigma(\textbf{h}^{(1)}\mathbf{\Theta^{2}})$ \\
    	$\textbf{h}^{(2)}=\mathbf{[1 \hspace{0.2cm}h^{(2)}]}$\\
    	$\textbf{h}^{(3)}=\sigma(\textbf{h}^{(2)}\mathbf{\Theta^{3}})$ \\
    	$\vdots$\\
    	$\textbf{h}^{(n-2)}=\sigma(\textbf{h}^{(n-3)}\mathbf{\Theta^{n-2}})$ \\
    	$\textbf{h}^{(n-2)}=\mathbf{[1 \hspace{0.2cm}h^{(n-2)}]}$\\
    	$\textbf{H}_{\mathbf{\Theta}}=\sigma(\textbf{h}^{(n-2)}\mathbf{\Theta^{n-1}})$ \\
    	$p^{(i)}$ is the number of nodes in the $i^{th}$ layer. Thus $p^{(n)}$ is the number of output nodes,
    	$n$ is the number of layers in the network (input+hidden+output)
    	$J$
    \end{frame} 

  \begin{frame}
  	\frametitle{Batch Gradient Descent vs Stochastic Gradient Descent}
    	$\textbf{X}=\textbf{[1 X]}$\\
    	$\textbf{h}^{(1)}=\sigma(\mathbf{X}\mathbf{\Theta^{1}})$ \\
    	$\textbf{h}^{(1)}=\mathbf{[1 \hspace{0.2cm}h^{(1)}]}$\\
    	$\textbf{h}^{(2)}=\sigma(\textbf{h}^{(1)}\mathbf{\Theta^{2}})$ \\
    	$\textbf{h}^{(2)}=\mathbf{[1 \hspace{0.2cm}h^{(2)}]}$\\
    	$\textbf{h}^{(3)}=\sigma(\textbf{h}^{(2)}\mathbf{\Theta^{3}})$ \\
    	$\vdots$\\
    	$\textbf{h}^{(n-2)}=\sigma(\textbf{h}^{(n-3)}\mathbf{\Theta^{n-2}})$ \\
    	$\textbf{h}^{(n-2)}=\mathbf{[1 \hspace{0.2cm}h^{(n-2)}]}$\\
    	$\textbf{H}_{\mathbf{\Theta}}=\sigma(\textbf{h}^{(n-2)}\mathbf{\Theta^{n-1}})$ \\
    	$p^{(i)}$ is the number of nodes in the $i^{th}$ layer. Thus $p^{(n)}$ is the number of output nodes,
    	$n$ is the number of layers in the network (input+hidden+output)			
  \end{frame}
  
    \begin{frame}
    	\frametitle{Batch Gradient Descent vs Stochastic Gradient Descent}
    	$J(\Theta)=-\frac{1}{m}\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K} \left[y_{k}^{(i)}log(H_{\Theta}(x^{(i)})_{k})  (1-y_{k}^{(i)})log(1-H_{\Theta}(x^{(i)})_{k}) \right]$\\
    	$J(\Theta)=-\frac{1}{m}\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K} \left[y_{k}^{(i)}-(H_{\Theta}(x^{(i)})_{k})^{2} \right]$	
    	$J(\Theta)=J(\Theta)+\frac{\lambda}{2m} \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{p^{(i)}-1} \sum\limits_{q=1}^{p^{(i)}}(\Theta_{q,j}^i)^{2} $\\			
    \end{frame}
\end{document}
