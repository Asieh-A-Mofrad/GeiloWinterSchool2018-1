{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import random\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "from sklearn import tree \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "<b>Question :  If you were to draw a line, which line would you choose to draw so as to accurately classify the two binary linearly separable classes? and Why? Hint: Margin. </b> \n",
    "\n",
    "<img src='./images/svm1.png'> \n",
    "\n",
    "Concept of Margin. SVM follows this intuition and hence is called a maximum margin classifer.\n",
    "\n",
    "<img src='./images/svm2.png'>   <img src='./images/svmmargin.png'>\n",
    "\n",
    "<b>Question - how does maximum margin concept help with accuracy </b>\n",
    "\n",
    "This has to do with overfitting (variance) vs underfitting (bias). The primary reason for having decision boundaries with large margins is that they tend to have a lower generalization error whereas models with small margins are more prone to overfitting.\n",
    "\n",
    "<b>Question - How does SVM finds this margin. What is Support Vector and what is Hyperplane? </b>\n",
    "\n",
    "In order to identify this margin, SVM follows the following routes :\n",
    "\n",
    "1.  SVM searches for the closest points, which it calls the \"support vectors\" . The name \"support vector machine\" is due to the fact that these points are like vectors and that the line is \"supported by\" the closest points. \n",
    "\n",
    "<img src='./images/svmhow.png'> \n",
    "\n",
    "2. The SVM draws a line connecting the support vectors (see the line labeled 'w' in Figure 2). It draws this connecting line by doing vector subtraction (point A - point B). The SVM declares the best separating line to be the line that bisects and is perpendicular to the connecting line. This is defined mathematically below.\n",
    "\n",
    "<img src='./images/svm3.png'> \n",
    "<img src='./images/nonlinear.png'> \n",
    "\n",
    "In 2 dimensions - Line.\n",
    "\n",
    "In 3 dimensions - Plane.\n",
    "\n",
    "In m dimensions - Hyperplane with m-1 dimension.\n",
    "\n",
    "\n",
    "\n",
    "Thus,  Support Vector Machine (SVM) is primarily a classier method that performs classification tasks by constructing hyperplanes in a multidimensional space that separates cases of different class labels. However,  Support Vector Machine can be applied to regression problems.\n",
    "\n",
    "<b>Reiterate the Basic Intuition behind SVM:</b> \n",
    "SVM focus  on support vectors (the data points that are the most difficult to tell apart), whereas other classifiers pay attention to all of the points. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier.\n",
    "\n",
    "<b>Reason:</b> \n",
    "If a classifier/regressor is good at the most challenging comparisons, then the classifier will be even better at the easy comparisons. When you get a new sample (new points), it is less likely to get misclassified as SVM has already made a line that keeps the most challenging classes as far away from each other as possible, and so it is less likely that one will spillover across the line into the other's territory. \n",
    "\n",
    "\n",
    "<b>Mathematically</b>\n",
    "\n",
    "For the linearly separable case, any point $\\mathbf{x}$ lying on the separating hyperplane satisfies the equation:\n",
    "\n",
    "$\\mathbf{x}^T\\mathbf{w} + b = 0$ , \n",
    "\n",
    "where $\\mathbf{w}$ is the vector normal to the hyperplane and $b$ is a constant that describes how much plane is shifted relative to the origin. \n",
    "\n",
    "The SVM should do two things a. maximize the margin (the distance between the two hyperplanes - the positive and negative hyperplanes -that are passing through nearest support vectors) , and b. Ensure that classification is correct using constraints. The positive and negative hyperplanes are parallel to the decision boundary. These hyperplanes are mathematically defined as : \n",
    "\n",
    "$\\mathbf{x}_{negative}^T\\mathbf{w} + b = -1$ \n",
    "\n",
    "and \n",
    "\n",
    "$\\mathbf{x}_{positive}^T\\mathbf{w} + b = 1$ \n",
    "\n",
    "\n",
    "This margin distance between these two hyperplanes is equal to  $$\\frac{2}{\\|\\mathbf{w}\\|}$$ \n",
    "\n",
    "This means that the objective is to solve $$\\underset{\\mathbf{w}}{\\operatorname{max}} \\frac{2}{\\|\\mathbf{w}\\|}$$\n",
    "\n",
    "Equivalently we want $$\\underset{\\mathbf{w}}{\\operatorname{min}} \\frac{\\|\\mathbf{w}\\|}{2}$$\n",
    "\n",
    "<img src='./images/svmclass.png'> \n",
    "\n",
    "<b>How do we get to to the objective equation :</b> \n",
    "  \n",
    "If we subtract the two linear equations representing the hyperplane from each other, we get:\n",
    "\n",
    "$$w^T \\left( x_{positive}-x_{negative} \\right) = 2$$\n",
    "\n",
    "Normalizing by $$\\Vert w \\Vert = \\sum_{j=1}^m w_j^2$$ on both sides by summing over 'm' number of support vector points. we get the equation,\n",
    "\n",
    "$$\\frac {w^T \\left( x_{positive}-x_{negative} \\right)} {\\Vert w \\Vert} = \\frac {2} {\\Vert w \\Vert}$$\n",
    "\n",
    "The LHS of the equation below can then be interpreted as the distance between the positive and negative hyperplane, which is the margin that we want to maximize. \n",
    "\n",
    "Now the objective function of the SVM becomes the maximization of this margin \n",
    "$$2 \\Vert w \\Vert$$\n",
    "with the constraint of the samples being classified correctly. \n",
    "In other words, all negative samples should fall on one side of the negative hyperplane while all the positive samples should fall behind the positive hyperplane. The constraints are mathematically defined as :\n",
    "\n",
    "$$b +w^Tx^{(i)} = \\begin{cases} \\ge 1 & \\quad if \\; y^{(i)}=1 \\\\ \\lt -1 & \\quad if \\; y^{(i)}=-1 \\end{cases}$$\n",
    "\n",
    "$$y^{(i)} \\left ( b+w^Tx^{(i)} \\right ) \\ge 1 \\quad \\forall i$$\n",
    "\n",
    "These equations are solved to obtain the support vectors and the weights.\n",
    "\n",
    "and final prediction is done by using\n",
    "$$y^{(i)}=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  -1 &\\text{ if } \\mathbf{w^T}\\mathbf{x}^{(i)}+b \\leq -1 \\\\\n",
    "                  1 &\\text{ if } \\mathbf{w^T}\\mathbf{x}^{(i)}+b \\ge 1 \\\\\n",
    "                \\end{array}\n",
    "              \\right.$$\n",
    "\n",
    "The maximal margin classifier is a very natural way to perform classification, if a separating hyper plane exists. However, in most real-life cases no separating hyper plane exists, and so there is no maximal margin classifier.\n",
    "\n",
    "<b> The reason for introducing the slack variable $$Î¾$$ </b>\n",
    "\n",
    "<img src='./images/svmslack.png'> \n",
    "\n",
    "Assuming the classes overlap in the given feature space. One way to deal with the overlap is to still maximize M, but allow for some points to be on the wrong side of the margin. In order to allow these, we can define the slack variables as, $$\\xi = ( \\xi_1, \\xi_2 \\ldots \\xi_m)$$ . In such circumstances, the linear constraints need to be relaxed for nonlinearly separable data to allow convergence of the optimization in the presence of misclassifications under the appropriate cost penalization.\n",
    "\n",
    "The slack variable simply can be added to the linear constraints:\n",
    "\n",
    "$$w^Tx^{(i)} = \\begin{cases} \\ge 1 & \\quad if \\; y^{(i)}=1-\\xi^{(i)} \\\\ \\lt -1 & \\quad if \\; y^{(i)}=-1 +\\xi^{(i)}\\end{cases}$$\n",
    "\n",
    "So, the new objective to be minimized becomes:\n",
    "\n",
    "$${\\Vert w \\Vert}^2 + C \\left ( \\sum_i \\xi^{(i)} \\right )$$\n",
    "\n",
    "\n",
    "With the variable C, we can penalize for misclassification. We can then use the parameter C to control the width of the margin and therefore tune the bias-variance trade-off.\n",
    "\n",
    "Large values of C correspond to large error penalties . With smaller values of C, we are less strict about misclassification errors.\n",
    "\n",
    " \n",
    "<b>Kernal Trick </b>\n",
    "\n",
    "What is a kernel and what is the need for it?\n",
    "\n",
    "For non-linear separable data. \n",
    "\n",
    "A dataset that is not linearly separable in lower dimensions (original feature space dimension) may be linearly separable in a higher-dimensional space. Thus, if we have a transformation that lifts the dataset to a higher-dimensional such that it is linearly separable, then we can train a linear SVM on  to find a decision boundary that separates the classes in the higher dimension. Projecting the decision boundary  found in higher dimension  back to the original space  will yield a nonlinear decision boundary. This means that we can learn nonlinear SVMs while still using the original Linear SVM\n",
    "formulation. Example :\n",
    "\n",
    "<img src='./images/3.png'> \n",
    "\n",
    "However , transforming from lower dimensional to higher dimensional incurs serious computational and memory problems.\n",
    "\n",
    "The kernel trick offers a solution.\n",
    "\n",
    "It turns out that there exist functions that, given two vectors v and w in lower dimensions,  implicitly computes the dot product between v and w in a higher-dimensional without explicitly transforming v and w to higher dimensions . Such functions are called kernel  functions. What does it mean?\n",
    "\n",
    "The implications are: By using a kernel, we can implicitly transform datasets to a higher-dimensional using no extra memory, and with a minimal effect on computation time. The only effect on computation is the extra time required to compute the kernel. Depending on kernel function chosen, this can be minimal. By virtue of kernel trick, we can efficiently  learn nonlinear decision boundaries for SVMs simply by replacing all dot products in the SVM computation with kernel function. \n",
    "\n",
    "\n",
    "<b>Advantages and Disadvantages:</b>\n",
    "\n",
    "<b>Advantages</b>\n",
    "\n",
    "Control over Generalization Ability of algorithm through placement of hyperplane\n",
    "Avoid difficulties of using linear functions in the high dimensional feature\n",
    "Accuracy\n",
    "Works well on smaller cleaner datasets\n",
    "It can be more efficient because it uses a subset of training points\n",
    "\n",
    "\n",
    "<b>Disadvantages</b>\n",
    "Computaional cost is prohibhitive with larger datasets as the training time with SVMs can be high\n",
    "It can be less effective on noisier datasets with overlapping classes\n",
    "\n",
    "\n",
    "<b>Parameters:</b>\n",
    "\n",
    "C :  Penalty parameter C of the error term.\n",
    "\n",
    "epsilon : Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.\n",
    "\n",
    "kernel :  Specifies the kernel type to be used in the algorithm. It must be one of âlinearâ, âpolyâ, ârbfâ, âsigmoidâ, âprecomputedâ or a callable. If none is given, ârbfâ will be used. If a callable is given it is used to precompute the kernel matrix.\n",
    "\n",
    "degree : Degree of the polynomial kernel function (âpolyâ). Ignored by all other kernels.\n",
    "\n",
    "gamma : Kernel coefficient for ârbfâ, âpolyâ and âsigmoidâ. If gamma is âautoâ then 1/n_features will be used instead.\n",
    "\n",
    "coef0 : Independent term in kernel function. It is only significant in âpolyâ and âsigmoidâ.\n",
    "\n",
    "\n",
    "tol :  Tolerance for stopping criterion.\n",
    "\n",
    "\n",
    "\n",
    "<b>Popular parameter-tuning technique:</b>\n",
    " \n",
    " K Fold cross-validation\n",
    " \n",
    " \n",
    " \n",
    "<b>Examples</b>\n",
    " \n",
    "<b> Simple Example - Iris Classification</b>\n",
    " \n",
    " \n",
    " Real Industrial Regression Example at SINTEF\n",
    " \n",
    " \n",
    " Real Industrial Classification Example at SINTEF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('./data/microchip.data', delimiter=\",\")\n",
    "X = data[:,:2]\n",
    "Y = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prettyPicture(clf, X_test, y_test):\n",
    "    #x_min = 0.0; x_max = 1.0\n",
    "    #y_min = 0.0; y_max = 1.0\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .01  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(14,14))\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    plt.pcolormesh(xx, yy, Z)#, cmap=pl.cm.seismic)\n",
    "\n",
    "    # Plot also the test points\n",
    "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "\n",
    "    plt.scatter(grade_sig, bumpy_sig, color = \"r\", label=\"fast\")\n",
    "    plt.scatter(grade_bkg, bumpy_bkg, color = \"b\", label=\"slow\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"bumpiness\")\n",
    "    plt.ylabel(\"grade\")\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the other hyperparameters\n",
    "\n",
    "DecisionTreeClassifier(criterion=âginiâ, splitter=âbestâ, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/collections.py:549: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == 'face':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86440677966101698"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, svm, pipeline\n",
    "clf=svm.SVC(kernel='rbf',C=1e3, gamma=0.1)\n",
    "clf.fit(X,Y)\n",
    "prettyPicture(clf, X, Y);\n",
    "y_predict=clf.predict(X)\n",
    "accuracy_score(Y,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hypertune(estimator,tuned_parameters,cv1,X_train,y_train,y_test):\n",
    "    #print (estimator,tuned_parameters,cv1)\n",
    "    clf = GridSearchCV(estimator,tuned_parameters,cv=cv1)\n",
    "#,scoring    \n",
    "    clf.fit(X_train,y_train)\n",
    "    print(\"Coefficients based on fit:\")\n",
    "    print()\n",
    "    #print(\"Coefficient: \", clf.best_estimator_.steps[-1][1].coef_)   \n",
    "    print()\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    #print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The R2 scores are computed on the full evaluation set, and is.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(r2_score(y_true, y_pred))\n",
    "    print()\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator,outfile,title, X, y, ylim=None, cv=None,n_jobs=1,train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(outfile,bbox_inches='tight')\n",
    "    return (plt,train_sizes, train_scores, test_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_measured_prediction(title,y_test,predicted,out_file):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, predicted, edgecolors=(0, 0, 0))\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.title(title)\n",
    "    plt.savefig(out_file,bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function takes a model parameter and a ranger and produces a plot and dataframe of CV scores for the parameter values\n",
    "def evaluate_param(parameter, num_range, index,clf,y_train):\n",
    "    grid_search = GridSearchCV(clf, param_grid = {parameter: num_range})\n",
    "    grid_search.fit(X_train, y_train) #features\n",
    "    \n",
    "    df = {}\n",
    "#    grid_search.cv\n",
    "#    for i, score in enumerate(grid_search.grid_scores_):\n",
    "#        df[score[0][parameter]] = score[1]\n",
    " \n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    print('means',means,num_range)\n",
    "#from operator import itemgetter\n",
    "#paramvalues=map(itemgetter(1),grid_search.cv_results_['params'])\n",
    "#print('paramvalues',paramvalues)\n",
    "#grid_search.cv_results_['params'].values()))\n",
    "    \n",
    "#    for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    plt.subplot(2,2,index)\n",
    "    plot = plt.plot(num_range,means)\n",
    "    plt.title(parameter)\n",
    "    return plot\n",
    "#    df = pd.DataFrame.from_dict(df, orient='index')\n",
    "#    df.reset_index(level=0, inplace=True)\n",
    "#    df = df.sort_values(by='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'S300099new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-912235c104ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msvra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mparam_gridrf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"svra__C\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"svra__gamma\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS300099new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'S300099new' is not defined"
     ]
    }
   ],
   "source": [
    "#ESTIMATE ONE PARAMETER AT A TIME \n",
    "svra=SVC(kernel='rbf',C=1e3, gamma=0.1)\n",
    "param_gridrf={\"svra__C\":np.arange(0.1,100,20),\"svra__gamma\": np.arange(0.1,100,20)}\n",
    "features = S300099new.columns \n",
    "print('feature',features)\n",
    "\n",
    "#y_t=y_train[:,0].ravel()\n",
    "\n",
    "#print('trainsize',X_train.shape)\n",
    "\n",
    "#print('trainsize',y_t.shape) index = 1 plt.figure(figsize=(16,12)) for parameter, param_range in dict.items(param_gridrf):\n",
    "for parameter, param_range in dict.items(param_gridrf):\n",
    "    evaluate_param(parameter, param_range, index,rfr1,y_t)\n",
    "    index += 1 \n",
    "    import matplotlib.pylab as pylab \n",
    "    params = {'legend.fontsize': 'small',\n",
    "      #'figure.figsize': (15, 5),\n",
    "     'axes.labelsize': 'small',\n",
    "     'axes.titlesize':'small',\n",
    "     'xtick.labelsize':'small',\n",
    "     'ytick.labelsize':'small'}\n",
    "    pylab.rcParams.update(params) \n",
    "    plt.rcParams['font.size']=24\n",
    "    plt.rcParams['axes.labelsize']=24 \n",
    "    plt.savefig('randomforestparavarclass2silica.pdf',bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svra=SVR(kernel='rbf',C=1e3, gamma=0.1)\n",
    "param_grid={\"svra__C\":[10,100],\"svra__gamma\": [0.1,10,50]}\n",
    "#tuned_parameters = [{'kernel': ['rbf'],'svra__C':[1, 10, 100, 1000]}]\n",
    "#param_gridsv = [\n",
    " # {'svr__C': [1, 10, 100], 'kernel': ['linear']},\n",
    "# {'C': [200,500,1000], 'gamma': [1,10,50], 'kernel': ['rbf']},\n",
    "# ]\n",
    "y_t=y_train.ravel()\n",
    "#kernel='rbf', C=1e3, gamma=0.1)\n",
    "estimatorsv = Pipeline(steps = [       \n",
    "    ('feature_processing', pipeline.FeatureUnion(transformer_list=transformer_list)),\n",
    "    ('svra',svra)\n",
    "])\n",
    "#print(estimatorsv.get_params().keys())\n",
    "print('svm hypertune')\n",
    "grid_cvsvr=hypertune(estimatorsv,param_grid,3,X_train,y_t)\n",
    "#print (grid_cvsvr.best_estimator_)\n",
    "\n",
    "y_predictedSVM=grid_cvsvr.predict(X_test)\n",
    "\n",
    "\n",
    "title = \"Learning Curves (SVM)\"\n",
    "out_file1=('SVMLC.pdf')\n",
    "plot_learning_curve(grid_cvsvr.best_estimator_,out_file1,title,X_train,y_t, ylim=None,cv=3,n_jobs=1)\n",
    "\n",
    "\n",
    "print (\"SVR Daily error of trip count:\", median_absolute_error(y_test,grid_cvsvr.predict(X_test)))\n",
    "title = \"SVR Fit measured Vs predicted\"\n",
    "out_file2=('SVMFit.pdf')\n",
    "plot_measured_prediction(title,y_test,y_predictedSVM,out_file2)\n",
    "                              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
